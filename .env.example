# MCP Client - Environment Variables Configuration

# =============================================================================
# OpenAI Configuration (for cloud-based LLM)
# =============================================================================
# Required if using OpenAI provider
# Get your key from: https://platform.openai.com/api-keys
#OPENAI_API_KEY=sk-...

# Optional: Override default OpenAI model (default: gpt-4o-mini)
#OPENAI_MODEL=gpt-4o-mini
# Other options: gpt-4o, gpt-4-turbo, gpt-3.5-turbo

# =============================================================================
# Ollama Configuration (for local LLM)
# =============================================================================
# Optional: Override default Ollama model (default: qwen2.5:7b)
#OLLAMA_MODEL=qwen2.5:7b

# Recommended models for MacBook M3:
# - qwen2.5:7b (default) - Best balance (4.7GB RAM, ~25 tokens/s)
# - llama3.2:3b - Fast and efficient (2GB RAM, ~45 tokens/s)
# - llama3.2:1b - Ultra lightweight (1GB RAM, ~90 tokens/s)
# - phi4:14b - Highest quality (8GB RAM, ~12 tokens/s)

# Optional: Override Ollama server URL (default: http://localhost:11434)
#OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# Usage Instructions
# =============================================================================
# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Uncomment and fill in the values you need
#
# 3. Load variables in your shell:
#    source .env
#    # or
#    export $(cat .env | xargs)
#
# 4. Run the client:
#    python3 -m mcp_client examples/mcp_server/mcp_server.py --chat
#
# =============================================================================
# Provider Selection Logic
# =============================================================================
# The client automatically selects the provider in this order:
# 1. If --provider CLI flag is specified, use that
# 2. If OPENAI_API_KEY is set, use OpenAI
# 3. Otherwise, use Ollama (local)
#
# Examples:
# 
# Auto-detect (uses Ollama if no OpenAI key):
#   python3 -m mcp_client server.py --chat
#
# Force OpenAI:
#   python3 -m mcp_client server.py --chat --provider openai
#
# Force Ollama with specific model:
#   python3 -m mcp_client server.py --chat --provider ollama --model llama3.2:3b
#
